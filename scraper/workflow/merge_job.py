
import logging
import os
import sys

import pandas as pd

sys.path.append('../')

import scraper.configuration as config


def merge_with_data_to_keep(translated_data):
    """
    Merges preprocessed data with data that hasn't changed since the last scraper run

    Parameters
    ----------
    translated_data : df
        Dataframe of the merged translated data (all languages combined)

    Input
    ----------
    data_to_keep.pkl: pkl
        Data that hasn't changed since the last scraper run

    Output 
    ----------
    merged_data.pkl : pickle
        Outputs a pickle file of the merged dataframes which is uploaded as artifact to github
    """
    data_to_keep = pd.read_pickle(os.path.join(os.path.split(config.GEOSERVICES_CH_CSV)[0],'data_to_keep.pkl'))
    merged_database = pd.concat([data_to_keep, translated_data], axis=0)
    print(f"Merged database has {len(merged_database.index)} rows, saving to pickle...")
    merged_database = merged_database.replace(to_replace='nan', value="", regex=True) # replace nan with empty string!
    merged_database.to_pickle(os.path.join(os.path.split(config.GEOSERVICES_CH_CSV)[0],'merged_data.pkl'))
    merged_database.to_csv(os.path.join(os.path.split(config.GEOSERVICES_CH_CSV)[0],'merged_data.csv'))
    print("\n Merge completed")
    logger.info("Merge completed")


if __name__ == "__main__":
    """
    Is triggered by github action pipeline once all language files has been successfully generated by the translation stage.
    """
    # Initialize and configure the logger
    logger = logging.getLogger("Scraping log")
    logger.setLevel(logging.INFO)
    fh = logging.FileHandler(config.LOG_FILE, "w", "utf-8")
    fh.setLevel(logging.INFO)
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(filename)s >"
                                  "%(funcName)17s(): Line %(lineno)s - "
                                  "%(levelname)s - %(message)s")
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    lang_found = []
    # Detect pickle files for languages dynamically
    for lang in config.WORKFLOW_TRANSLATE_LANGUAGES:
        file_path = os.path.join(config.WORKFLOW_ARTIFACT_FOLDER, '{}_translated.pkl'.format(lang))
        if os.path.exists(file_path):
            lang_found.append(lang)

    # Process and merge files
    if len(lang_found) < 1:
        print(f"Merge not possible, {lang_found} languages found" )

    if len(lang_found) == 1:
        df1 = pd.read_pickle(os.path.join(config.WORKFLOW_ARTIFACT_FOLDER,  '{}_translated.pkl'.format(lang)))
        print(f"First language has {len(df1.index)} rows")

        merge_with_data_to_keep(df1)

    if len(lang_found) > 1:
        df1 = pd.read_pickle(os.path.join(config.WORKFLOW_ARTIFACT_FOLDER,  '{}_translated.pkl'.format(lang_found[0])))
        for lang in lang_found[1:]: # start from the second language
            translated_columns_to_add = config.WORKFLOW_MERGE_COLUMNS + [col_name + "_" + lang for col_name in config.WORKFLOW_TRANSLATE_COLUMNS]
            df2 = pd.read_pickle(os.path.join(config.WORKFLOW_ARTIFACT_FOLDER,  '{}_translated.pkl'.format(lang)))
            df1 = df1.merge(df2[translated_columns_to_add], on=config.WORKFLOW_MERGE_COLUMNS)
        
        print(f"All languages combined have {len(df1.index)} rows")
        merge_with_data_to_keep(df1)
