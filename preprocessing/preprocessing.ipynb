{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ma1021525\\Anaconda3\\envs\\geoharvester\\lib\\site-packages\\torch\\__init__.py:749: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:433.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "sys.path.append(\"\\\\\".join(os.getcwd().split(\"\\\\\")[:-1]))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from scraper import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_github_repo = \"https://github.com/FHNW-IVGI/Geoharvester/blob/main/scraper/data/\"\n",
    "url_github_repo_suffix = \"?raw=true\"\n",
    "url_geoservices_CH_csv = \"{}geoservices_CH.csv{}\".format(url_github_repo,url_github_repo_suffix)\n",
    "fields_to_include = [\"provider\",\"title\", \"keywords\", \"abstract\", \"service\", \"endpoint\", \"preview\"]\n",
    "\n",
    "def load_data(rawdata=False):\n",
    "    if rawdata:\n",
    "        dataframe = pd.read_csv(url_geoservices_CH_csv, low_memory=False)\n",
    "    else:\n",
    "        dataframe = pd.read_csv(url_geoservices_CH_csv, usecols=fields_to_include, low_memory=False)\n",
    "    return dataframe\n",
    "\n",
    "# dataframe_some_cols = load_data()\n",
    "# database = dataframe_some_cols.fillna(\"nan\")\n",
    "dataframe_some_cols = load_data(rawdata=True)\n",
    "database_raw = dataframe_some_cols.fillna(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv(\"./data/geoservices_CH.csv\", low_memory=False)\n",
    "def count(row, exc=0):\n",
    "    try:\n",
    "        num = len(str(row).split())\n",
    "    except TypeError:\n",
    "        num = exc\n",
    "    except AttributeError:\n",
    "        num = exc\n",
    "    if pd.isna(row):\n",
    "        num = 0\n",
    "    return num\n",
    "db['abstract_w_count'] = db['ABSTRACT'].apply(count)\n",
    "db['keywords_w_count'] = db['KEYWORDS'].apply(count)\n",
    "db.replace(['Bund','FL_LI','Geodienste','KT_AG','KT_AI','KT_AR','KT_BE','KT_BL','KT_BS','KT_FR','KT_GE','KT_GL','KT_GR','KT_JU','KT_SG','KT_SH','KT_SO','KT_SZ','KT_TG','KT_TI','KT_UR','KT_VD','KT_ZG','KT_ZH'],\n",
    "              ['Bund','LI','Geodienste','AG','AI','AR','BE','BL','BS','FR','GE','GL','GR','JU','SG','SH','SO','SZ','TG','TI','UR','VD','ZG','ZH'], inplace=True)\n",
    "db_nan = db[db['OWNER'] == 'Bund'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[(db['OWNER'] == 'AI') & (db['keywords_w_count'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in db.columns.to_list():\n",
    "    if (db[column][db[column].isna()]).empty:\n",
    "        print(f\"No field missing in {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gr = db.groupby(['OWNER']).count()[['TITLE','ABSTRACT', 'KEYWORDS', 'NAME']]\n",
    "db_gr['abstract_perc'] = 100 / db_gr['TITLE'] * db_gr['ABSTRACT']\n",
    "db_gr['keywords_perc'] = 100 / db_gr['TITLE'] * db_gr['KEYWORDS']\n",
    "db_gr['name_perc'] = 100 / db_gr['TITLE'] * db_gr['NAME']\n",
    "db_gr.rename({'Geodienste':'Geodnst'}, inplace=True)\n",
    "db_gr.sort_values(by=['OWNER'], ascending=False, inplace=True)\n",
    "db_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, db_gr['TITLE'])\n",
    "plt.title(\"Number of OWS\")\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.barh(db_gr.index, db_gr['abstract_perc'])\n",
    "plt.title(\"Abstact filled %\", fontsize=22)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":18})\n",
    "plt.xlim(0,100)\n",
    "# plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.barh(db_gr.index, db_gr['keywords_perc'], )\n",
    "plt.title(\"Keywords filled %\", fontsize=22)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":18})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\", fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 / db['abstract_w_count'].count() * db[db['abstract_w_count'] > 20]['ABSTRACT'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db['abstract_w_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = db[['ABSTRACT', 'KEYWORDS', 'OWNER', 'abstract_w_count','keywords_w_count', 'TITLE']]\n",
    "test = test.groupby(['OWNER']).mean()[['abstract_w_count','keywords_w_count']]\n",
    "test.rename({'Geodienste':'Geodnst'}, inplace=True)\n",
    "test.sort_values(by=['OWNER'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test.groupby(['OWNER','keywords_w_count']).count()['TITLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(test.index, test['abstract_w_count'], )\n",
    "plt.title(\"Abstract Wordcount Median\")\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "# plt.vlines(x=1.0, ymin=0, ymax=len(test.index.to_list())-0.5, color='grey')\n",
    "plt.barh(test.index, test['keywords_w_count'], )\n",
    "plt.title(\"Keywords average word count\", fontsize=22)\n",
    "plt.autoscale(tight=True)\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.rcParams.update({\"font.size\":18})\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing functions from utils.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement of the TF-IDF with BM25 to execute queries on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstracts = result['abstract'].values.tolist()\n",
    "bm25 = utils.TFIDF_BM25()\n",
    "bm25.cleansing_ranking(database, column='abstract') # 1421 lines in 36 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the vector to the database\n",
    "bm25.fit()\n",
    "# search the best match in the vector\n",
    "res = bm25.search('Brandmeldeanlage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the keyword extraction with spacy (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = utils.NLP_spacy()\n",
    "# Keyword\n",
    "keywords_NLP = NLP.extract_keywords(database, column='abstract') # with small models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_NLP[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original abstract\n",
    "database['abstract'].values.tolist()[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LSA and LSI with gensim for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSA = utils.LSI_LDA()\n",
    "abstracts = LSA.preprocess(database)\n",
    "LSA.compute_coherence_values_LSI((4,16,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_number_topics = 12\n",
    "number_of_words = 20\n",
    "lsamodel = LSA.create_gensim_lsa_model(best_number_topics, number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsamodel.print_topics(best_number_topics, number_of_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Text Summarization and Latent Dirchlet allocation (LDA) for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/text-summarization-for-clustering-documents-2e074da6437a\n",
    "# https://towardsdatascience.com/nlp-topic-modeling-to-identify-clusters-ca207244d04f\n",
    "\n",
    "# https://medium.com/plain-simple-software/build-an-ai-text-summarizer-in-python-6209fb23875d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = utils.NLP_spacy()\n",
    "keywords_dataset = NLP.extract_refined_keywords(database, use_rake=True, column='abstract',\n",
    "                                          keyword_length=3, num_keywords=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.loc[3]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the extracted keywords to the raw data dataframe\n",
    "def join_keywords(keywords_list):\n",
    "    keywords = ', '.join(kw for kw in keywords_list)\n",
    "    return keywords\n",
    "database_raw['keywords_nlp'] = list(map(join_keywords, keywords_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "st.title(\"Summarizer\")\n",
    "input_text = st.text_area(label='Enter full text:', value='', height=250)\n",
    "st.button(\"submit\")\n",
    "output_text = st.text_area(label='Summarized text:', value='', height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv('OPENAI_KEY') # WARNING: limits to 18$ free credits!\n",
    "\n",
    "def summarize_GPT(text):\n",
    "    prompt = f\"summarize this text: {text}\"\n",
    "    model = openai.Completion.create(model='text-davinci-003', prompt=prompt, temperature=.5, max_tokens=1000,)\n",
    "    return model[\"choices\"][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_GPT('''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. Hence, owing to the fact\n",
    "that it is a very innovative and, above all, safe solution, it is possible to determine the condition of the\n",
    "building, locate places exposed to thermal escape, and plan actions to improve the condition of the\n",
    "facility. The presented work is devoted to the technology of creating a dense point cloud and a 3D\n",
    "model, based on data obtained from UAV. It has been shown that it is possible to build a 3D point\n",
    "model based on thermograms with the specified accuracy by using thermal measurement marks and\n",
    "the dense matching method. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. The discussed approach exploits measurement data obtained with three\n",
    "independent devices (tools/appliances): a Matrice 300 RTK drone (courtesy of NaviGate); a Phantom\n",
    "4 PRO drone; and a KT-165 thermal imaging camera. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T5 abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/mt5-large\") #  google/mt5-large\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/mt5-large')\n",
    "device = torch.device('cpu')\n",
    "task_prefix = 'Summarize:'\n",
    "body = '''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. Hence, owing to the fact\n",
    "that it is a very innovative and, above all, safe solution, it is possible to determine the condition of the\n",
    "building, locate places exposed to thermal escape, and plan actions to improve the condition of the\n",
    "facility. The presented work is devoted to the technology of creating a dense point cloud and a 3D\n",
    "model, based on data obtained from UAV. It has been shown that it is possible to build a 3D point\n",
    "model based on thermograms with the specified accuracy by using thermal measurement marks and\n",
    "the dense matching method. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. The discussed approach exploits measurement data obtained with three\n",
    "independent devices (tools/appliances): a Matrice 300 RTK drone (courtesy of NaviGate); a Phantom\n",
    "4 PRO drone; and a KT-165 thermal imaging camera. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n",
    "tokenized_text = tokenizer.encode(task_prefix + body.strip().replace('\\n',''), return_tensors=\"pt\").to(device)\n",
    "summary = model.generate(tokenized_text, num_beams=1, no_repeat_ngram_size=1, min_length=20, max_length=80,\n",
    "                         early_stopping=True)\n",
    "output = tokenizer.decode(summary[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstractive summarization with SBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sentence-Bert to summarize the abstract (extractive summary)\n",
    "# https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models\n",
    "from summarizer.sbert import SBertSummarizer\n",
    "model = SBertSummarizer(model='paraphrase-multilingual-MiniLM-L12-v2') #all-MiniLM-L12-v2\n",
    "body = '''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. Hence, owing to the fact\n",
    "that it is a very innovative and, above all, safe solution, it is possible to determine the condition of the\n",
    "building, locate places exposed to thermal escape, and plan actions to improve the condition of the\n",
    "facility. The presented work is devoted to the technology of creating a dense point cloud and a 3D\n",
    "model, based on data obtained from UAV. It has been shown that it is possible to build a 3D point\n",
    "model based on thermograms with the specified accuracy by using thermal measurement marks and\n",
    "the dense matching method. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. The discussed approach exploits measurement data obtained with three\n",
    "independent devices (tools/appliances): a Matrice 300 RTK drone (courtesy of NaviGate); a Phantom\n",
    "4 PRO drone; and a KT-165 thermal imaging camera. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n",
    "summary = model(body, num_sentences=3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with paraphrase-multilingual v2\n",
    "'''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n",
    "# with all-MiniLM-L12-v2\n",
    "'''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. The discussed approach exploits measurement data obtained with three\n",
    "independent devices (tools/appliances): a Matrice 300 RTK drone (courtesy of NaviGate); a Phantom\n",
    "4 PRO drone; and a KT-165 thermal imaging camera.'''\n",
    "# all-mpnet-base-v2\n",
    "'''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. The results achieved in this way were compared and, as the result of\n",
    "this work, the model obtained from color photos was integrated with the point cloud created on the\n",
    "basis of the thermal images. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n",
    "# paraphrase-multilingual-MiniLM-L12-v2\n",
    "'''Thermal infrared imagery is very much gaining in importance in the diagnosis of energy\n",
    "losses in cultural heritage through non-destructive measurement methods. It has been shown that it is possible to build a 3D point\n",
    "model based on thermograms with the specified accuracy by using thermal measurement marks and\n",
    "the dense matching method. A stone church located in the southern part of\n",
    "Poland was chosen as the measuring object.'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA) with gensim for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = utils.LSI_LDA()\n",
    "texts_tokenized = LDA.preprocess(database, column='abstract') # ca. 5 min\n",
    "main_topics_lda = LDA.create_gensim_lda_model(categories='eCH') # ca. 3 min\n",
    "vis = LDA.prepare_plot_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic classification with TF-IDF vectors and predefined categories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INSPIRE categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/INSPIRE_categories.csv\", sep=';', encoding='UTF-16')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0][8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eCH categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/eCH_categories.csv\", sep=';', encoding='UTF-16')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic classification with TF-IDF vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = utils.TFIDF_BM25()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "# prepare the categories for INSPIRE\n",
    "cats_index = df.index.values\n",
    "# translate from english to german because the german description is too short\n",
    "full_desc_DE = [utils.translate(element, lang='de', translator='google') for element in df['complete_description'].tolist()]\n",
    "description_title_DE = [df['category_DE'].values.tolist()[i]+', '+full_desc_DE[i]\n",
    "                     for i in range(0, len(df))]\n",
    "stemmed_categories = [utils.tokenize_abstract(text, output_scores=False, stem_words=True) for\n",
    "                       text in description_title_DE]\n",
    "joined_categories = [' '.join(words) for words in stemmed_categories]\n",
    "vectorizer = bm25.vectorizer\n",
    "vectorizer.fit(joined_categories)\n",
    "score_categories = super(TfidfVectorizer, vectorizer).transform(joined_categories)\n",
    "doc_length = score_categories.sum(1).A1\n",
    "avd = score_categories.sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_keywords = [utils.stemming_sentence(text) for text in [' '.join(words) for words in keywords_dataset]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the database for the search (keywordds from rake in class NLP_spacy)\n",
    "# WARNING: the results aren't satisfactory\n",
    "import numpy as np\n",
    "dataset = 20\n",
    "k1 = 1.6\n",
    "b = 0.75\n",
    "for word in stemmed_keywords[dataset]:\n",
    "    query, = super(TfidfVectorizer, vectorizer).transform([word])\n",
    "    assert sparse.isspmatrix_csr(query)\n",
    "    kw = score_categories.tocsc()[:, query.indices]\n",
    "    idf = vectorizer._tfidf.idf_[None, query.indices] - 1.\n",
    "    scores = (kw.multiply(np.broadcast_to(idf, kw.shape)) * (k1 + 1)/ kw + (k1 * (1 - b + b * doc_length / avd))[:, None]).sum(1).A1\n",
    "    scores = [round(score, 2) for score in scores if score != 0.0 and not np.isnan(score)]\n",
    "    category = [cats_index[i] for i in range(0, len(scores)) if scores[i] > 0.0]\n",
    "    print(category, scores, word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_title_DE[17], description_title_DE[19], description_title_DE[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database.loc[dataset]['abstract']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible additional tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard/Cosine similarity -> for query search\n",
    "# WordNet-word-similarity -> for query search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add extracted data to raw data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract keywords from the abstract and add them to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword extraction\n",
    "NLP = utils.NLP_spacy()\n",
    "keywords_dataset = NLP.extract_refined_keywords(database, use_rake=True, refine_keywords=True,\n",
    "                                                 column='abstract', keyword_length=3, num_keywords=15)\n",
    "\n",
    "# Add the extracted keywords to the raw data dataframe\n",
    "def join_keywords(keywords_list):\n",
    "    keywords = ', '.join(kw for kw in keywords_list)\n",
    "    return keywords\n",
    "database_raw['keywords_nlp'] = list(map(join_keywords, keywords_dataset))\n",
    "# database_raw['keywords_rake'] = list(map(join_keywords, keywords_dataset))          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the abstracts with a SBert model and add them to the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = NLP.summarize_texts(database, column='abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summaries)\n",
    "database_raw['summary'] = summaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect and add the dataset's language based on titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {'english':('EN', 'ENG'), 'french':('FR','FRA'), 'german':('DE','DEU'), 'italian':('IT','ITA'), 'not_found':('NA','NAN')}\n",
    "database_raw['lang_3'] = database_raw.apply(lambda row: language_dict[utils.detect_language(row['title'], not_found=True)][1], axis=1)\n",
    "database_raw['lang_2'] = database_raw.apply(lambda row: language_dict[utils.detect_language(row['title'], not_found=True)][0], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check metadata quality and save it in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_raw = utils.check_metadata_quality(database_raw, search_word='nan',\n",
    "                                            search_columns=['abstract', 'keywords', 'metadata'],\n",
    "                                            case_sensitive=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the dataset for redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the str in the dataframe\n",
    "database_raw = database_raw.replace(to_replace=\"'\", value=\"-\", regex=True)\n",
    "database_raw = database_raw.replace(to_replace='\\\"', value=\"-\", regex=True)\n",
    "database_raw = database_raw.replace(to_replace=\"  \", value = \" \", regex=True)\n",
    "database_raw = database_raw.replace(to_replace=\"    \", value = \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database_raw.to_csv('rawdata_scraper.csv')\n",
    "database_raw.to_pickle('data/test_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_pickle('data/test_preprocessed.pkl')\n",
    "row = 201\n",
    "print(db['abstract'].loc[row])\n",
    "print(db['keywords_rake'].loc[row])\n",
    "print(db['keywords_nlp'].loc[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = pd.read_pickle(\"data/merged_data.pkl\")\n",
    "# new_db = pd.read_csv(\"data/geoservices_CH.csv\", low_memory=False)\n",
    "# new_db = new_db.fillna(\"nan\")\n",
    "# new_db = new_db.replace(to_replace=\"'\", value=\"-\", regex=True)\n",
    "# new_db = new_db.replace(to_replace='\\\"', value=\"-\", regex=True)\n",
    "# new_db = new_db.replace(to_replace=\"  \", value = \" \", regex=True)\n",
    "# new_db = new_db.replace(to_replace=\"    \", value = \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "translate_columns = [\"keywords_nlp\"]\n",
    "for translate_column in translate_columns:\n",
    "    for lang in ['en','fr','it']:\n",
    "        tlang = time()\n",
    "        new_col = translate_column+'_'+lang\n",
    "        if translate_column == 'title':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_text(\n",
    "                row[translate_column],to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'abstract':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_abstract(\n",
    "                row[translate_column], to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'keywords':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_keywords(\n",
    "                row[translate_column], to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'keywords_nlp':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_keywords(\n",
    "                row[translate_column].split(','), to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=True), axis=1)\n",
    "        else:\n",
    "            print(f\"Column {translate_column} could not be translated\")\n",
    "        print(f\"Transaltion of {translate_column} in {lang} required {(time()-tlang)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "translate_columns = [\"abstract\"]\n",
    "for translate_column in translate_columns:\n",
    "    for lang in ['it']:\n",
    "        tlang = time()\n",
    "        new_col = translate_column+'_'+lang\n",
    "        if translate_column == 'title':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_text(\n",
    "                row[translate_column],to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'abstract':\n",
    "            one_piece = ' | '.join(db.loc[0:10]['abstract'].to_list()).replace(\"| nan |\", \"\")\n",
    "            abstract_translated = utils.translate_abstract(one_piece,\n",
    "                                                           to_lang=lang, from_lang='DEU',\n",
    "                                                           use_api=True)\n",
    "            db[new_col] = abstract_translated.split(' | ')\n",
    "            # db[new_col] = db.progress_apply(lambda row: utils.translate_abstract(\n",
    "            #     row[translate_column], to_lang=lang, from_lang=row['lang_3'],\n",
    "            #     use_api=True), axis=1)\n",
    "        elif translate_column == 'keywords':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_keywords(\n",
    "                row[translate_column], to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=False), axis=1)\n",
    "        elif translate_column == 'keywords_nlp':\n",
    "            db[new_col] = db.progress_apply(lambda row: utils.translate_keywords(\n",
    "                row[translate_column].split(','), to_lang=lang, from_lang=row['lang_3'],\n",
    "                use_api=True), axis=1)\n",
    "        else:\n",
    "            print(f\"Column {translate_column} could not be translated\")\n",
    "        print(f\"Transaltion of {translate_column} in {lang} required {(time()-tlang)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(one_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.loc[10000:10200][['abstract','abstract_it']].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Abstract DE: {int(1280/60)} minutes\")\n",
    "print(f\"Abstract IT: {int(5750/60)} minutes\")\n",
    "print(f\"Abstract FR: {int(5092/60)} minutes\")\n",
    "print(f\"Abstract EN: {int(5141/60)} minutes\")\n",
    "\n",
    "print(f\"Title DE: {int(3738/60)} minutes\")\n",
    "print(f\"Title IT: {int(13998/60)} minutes\")\n",
    "print(f\"Title FR: {int(11284/60)} minutes\")\n",
    "print(f\"Title EN: {int(17649/60)} minutes\")\n",
    "\n",
    "print(f\"Keywords DE: {int(150/60)} minutes\")\n",
    "print(f\"Keywords IT: {int(1065/60)} minutes\")\n",
    "print(f\"Keywords FR: {int(1623/60)} minutes\")\n",
    "print(f\"Keywords EN: {int(1384/60)} minutes\")\n",
    "\n",
    "print(f\"Keywords_nlp DE: {int(4999/60)} minutes\")\n",
    "print(f\"Keywords_nlp IT: {int(16972/60)} minutes\")\n",
    "print(f\"Keywords_nlp FR: {int(19767/60)} minutes\")\n",
    "print(f\"Keywords_nlp EN: {int(24484/60)} minutes\")\n",
    "\n",
    "print(f\"\\nTotal ET: {(3738/60+13998/60+11284/60+17649/60+3738/60+13998/60+11284/60+17649/60+150/60+1065/60+1623/60+1384/60+4999/60+16972/60+19767/60+24484/60)/60} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transofrmers\n",
    "# sentencepiece\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\") #  google/mt5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.lang_code_to_id[\"de_DE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"The quick brown fox jumps over a lazy dog\"\n",
    "model_in = tokenizer(t, return_tensors=\"pt\")\n",
    "translated_tensors = model.generate(**model_in, forced_bos_token_id=tokenizer.lang_code_to_id[\"de_DE\"])\n",
    "translated_tokens = tokenizer.batch_decode(translated_tensors, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(translated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = utils.NLP_spacy()# missing keywords from abstract: 4803\n",
    "keywords_dataset = NLP.extract_refined_keywords(db[(db['keywords_nlp']=='')],\n",
    "                                                use_rake=True, refine_keywords=True,\n",
    "                                                column='title', keyword_length=3, num_keywords=15)\n",
    "k = 0\n",
    "for i, row in db[(db['keywords_nlp']=='')].iterrows():\n",
    "    if keywords_dataset[k]:\n",
    "        db.loc[i, 'keywords_nlp'] = \",\".join(keywords_dataset[k])\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = \"C:/Users/ma1021525/OneDrive - FHNW/Documents/Projects/git/Geoharvester/scraper/data/merged_data.pkl\"\n",
    "db = pd.read_pickle(pickle_path)\n",
    "# utils.translate(text, to_lang='de', translator='google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db[['provider','title','endpoint','metadata']][1500:1520].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in db[(db['lang_2']=='NA')&(db['provider']=='Bund')].iterrows():\n",
    "    db.loc[i, 'lang_2'] = 'DE'\n",
    "    db.loc[i, 'lang_3'] = 'DEU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict = {'english':('EN', 'ENG'), 'french':('FR','FRA'), 'german':('DE','DEU'), 'italian':('IT','ITA'), 'not_found':('NA','NAN')}\n",
    "for i, row in db[(db['lang_2']=='NA')&(db['keywords']!='nan')].iterrows():\n",
    "    lang = utils.detect_language(row['keywords'], not_found=True)\n",
    "    if lang[0] != 'not_found':\n",
    "        lan = language_dict[lang]\n",
    "        db.loc[i, 'lang_2'] = lan[0]\n",
    "        db.loc[i, 'lang_3'] = lan[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator, exceptions\n",
    "\n",
    "def translate_text(text, to_lang, from_lang):\n",
    "    language_dict = {'ENG':'en', 'FRA':'fr', 'DEU':'de', 'ITA':'it','NAN':'na'}\n",
    "    if language_dict[from_lang] == to_lang:\n",
    "        return text\n",
    "    else:\n",
    "        try:\n",
    "            trnd = GoogleTranslator(source='auto', target=to_lang).translate(text.replace('_',' '))\n",
    "        except exceptions.TranslationNotFound:\n",
    "            trnd = 'nan'\n",
    "        return trnd\n",
    "\n",
    "def translate_abstract(text, to_lang, from_lang):\n",
    "    language_dict = {'ENG':'en', 'FRA':'fr', 'DEU':'de', 'ITA':'it','NAN':'na'}\n",
    "    if to_lang != language_dict[from_lang] and text != 'nan':\n",
    "        if not text.startswith('http') or text.startswith('Link zu Metadaten:'):\n",
    "            try:\n",
    "                trnd = GoogleTranslator(source='auto', target=to_lang).translate(text.replace('_',' '))\n",
    "            except exceptions.TranslationNotFound:\n",
    "                trnd = 'nan'\n",
    "            return trnd\n",
    "        else:\n",
    "            return 'nan'\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "def translate_keywords(text, to_lang, from_lang):\n",
    "    if type(text) == str:\n",
    "        text = [text]\n",
    "    kwds = []\n",
    "    for kwd in text:\n",
    "        language_dict = {'ENG':'en', 'FRA':'fr', 'DEU':'de', 'ITA':'it','NAN':'na'}\n",
    "        if kwd != 'nan' and language_dict[from_lang] != to_lang:\n",
    "            if not kwd.startswith('http') or kwd.startswith('Link zu Metadaten:'):\n",
    "                try:\n",
    "                    kwd_trnsd = GoogleTranslator(source='auto', target=to_lang).translate(kwd.replace('_',' '))\n",
    "                    if not kwd_trnsd:\n",
    "                        kwd_trnsd = 'nan'\n",
    "                except exceptions.TranslationNotFound:\n",
    "                    kwd_trnsd = 'nan'\n",
    "            else:\n",
    "                kwd_trnsd = 'nan'\n",
    "            kwds.append(kwd_trnsd)\n",
    "        else:\n",
    "            kwds.append(kwd)\n",
    "    return ','.join(kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "languages = ['de']\n",
    "translate_column = 'keywords_nlp'#'title'#'abstract' # title # keywords\n",
    "\n",
    "for lang in languages:\n",
    "    print(f\"translating {translate_column} in {lang} ...\")\n",
    "    new_column = translate_column+'_'+lang\n",
    "    if translate_column == 'title':\n",
    "        db[new_column] = db.progress_apply(lambda row: translate_text(row[translate_column],to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "    elif translate_column == 'abstract':\n",
    "        db[new_column] = db.progress_apply(lambda row: translate_abstract(row[translate_column], to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "    elif translate_column == 'keywords':\n",
    "        db[new_column] = db.progress_apply(lambda row: translate_keywords(row[translate_column], to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "    else:\n",
    "        db[new_column] = db.progress_apply(lambda row: translate_keywords(row[translate_column].split(','), to_lang=lang, from_lang=row['lang_3']), axis=1)\n",
    "    db.to_pickle(pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'keywords'\n",
    "def count_links(field):\n",
    "    if field.startswith('http') or field.startswith('Link zu Metadaten:'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def count_nan(field):\n",
    "    if field == 'nan':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "db[f'{column}_link'] = db.progress_apply(lambda row: count_links(row[column]), axis=1)\n",
    "db[f'{column}_nan'] = db.progress_apply(lambda row: count_nan(row[column]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_gr = db.groupby(['provider'], numeric_only=True).sum()[['abstract_link', 'abstract_nan','keywords_link','keywords_nan']]\n",
    "db_gr.rename({'Geodienste':'Geodnst'}, inplace=True)\n",
    "db_gr.sort_values(by=['provider'], ascending=False, inplace=True)\n",
    "db_gr['count'] = db.groupby(['provider']).count()[['title']].sort_values(by=['provider'], ascending=False)['title'].to_list()\n",
    "db_gr['abstract'] = 100 / db_gr['TITLE'] * db_gr['ABSTRACT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * db_gr['abstract_link'])\n",
    "plt.title(\"Abstracts in % containing just a link\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * db_gr['abstract_nan'])\n",
    "plt.title(\"Empty abstracts in %\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * (db_gr['abstract_link']+db_gr['abstract_nan']))\n",
    "plt.title(\"Unusable abstracts in %\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * db_gr['keywords_link'])\n",
    "plt.title(\"Keywords in % containing just a link\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * db_gr['keywords_nan'])\n",
    "plt.title(\"Empty keywords in %\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.barh(db_gr.index, 100 / db_gr['count'] * (db_gr['keywords_link']+db_gr['keywords_nan']))\n",
    "plt.title(\"Unusable keywords in %\", fontsize=16)\n",
    "plt.autoscale(tight=True)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().spines['bottom'].set_visible(False)\n",
    "plt.tick_params(axis='y', which='both', left=False)\n",
    "plt.rcParams.update({\"font.size\":10})\n",
    "plt.xlim(0,100)\n",
    "plt.ylabel(\"Providers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://c8j9w8r3.rocketcdn.me/wp-content/uploads/2017/03/join-types-merge-names.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation ranking function\n",
    "- Results relevance evaluation\n",
    "- Results ranking evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'server'))\n",
    "from app.redis.methods import results_ranking, test_ranking\n",
    "\n",
    "db = pd.read_pickle('data/preprocessed_data.pkl')\n",
    "# db = db.rename(columns={'OWNER':'provider','TITLE':'title', 'NAME':'name','MAPGEO':'mapgeo','TREE':'tree','GROUP':'group',\n",
    "#                    'ABSTRACT':'abstract','KEYWORDS':'keywords','KEYWORDS_NLP':'keywords_nlp','LEGEND':'legend',\n",
    "#                    'CONTACT':'contact','ENDPOINT':'endpoint','METADATA':'metadata','UPDATE':'update','SERVICE':'service',\n",
    "#                    'MAX_ZOOM':'max_zoom','CENTER_LAT':'center_lat','CENTER_LON':'center_lon','BBOX':'bbox','SUMMARY':'summary',\n",
    "#                    'LANG_3':'lang_3','LANG_2':'lang_2','METAQUALITY':'metaquality'})\n",
    "# db = db.rename(columns={'SERVICETYPE':'servicetype','SERVICELINK':'servicelink'})\n",
    "# db.to_pickle('data/preprocessed_data.pkl')\n",
    "db = db[['provider','title','abstract','keywords','keywords_nlp','metaquality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time()\n",
    "db[(db['title'].str.contains('Winterthur', case=False)) | (db['keywords_nlp'].str.contains('winterthur', case=False))]# possiamo indicizzarlo essendo solo 6 keywords (verkehrsanordnung , stdte zrich, kanton zrich, winterthur, tempo30, begegnungszonen'), mentre invece l'abstract di 33 parole viene diffice e le keywords normali sono vuote! (dataset: 17705)\n",
    "# print(round(time()-t0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time()\n",
    "db[(db['title'].str.contains('Winterthur'))]\n",
    "# print(round(time()-t0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mun_ch_15K = pd.read_csv('./data/mun_ch_15K.csv', header=None)\n",
    "mun_ch_15K.columns = ['municipality']\n",
    "mun_ch_15K['OWS_discovered'] = 0\n",
    "mun_ch_15K['OWS'] = 0\n",
    "for mun in mun_ch_15K['municipality'].to_list():\n",
    "    t = db[(db['title'].str.contains(mun, case=False)) | (db['keywords_nlp'].str.contains(mun, case=False))]\n",
    "    mun_ch_15K.loc[mun_ch_15K['municipality']==mun, ['OWS']]=len(db[(db['title'].str.contains(mun, case=False))])\n",
    "    ows = len(t[~t.index.isin(db[(db['title'].str.contains(mun, case=False))].index.to_list())])#.loc[5311]['abstract'] # Also in this case THUN we have 5729 that is not discovered \n",
    "    mun_ch_15K.loc[mun_ch_15K['municipality']==mun, ['OWS_discovered']]=ows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mun_ch_15K[mun_ch_15K['OWS_discovered']<200]['OWS_discovered'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mun_ch_15K[(mun_ch_15K['OWS_discovered']<200)].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[(db['abstract'] != 'nan') &\n",
    "   (db['abstract'] != 'n.a.')&\n",
    "   (~db['abstract'].str.contains('@'))&\n",
    "   (db['provider'] == 'Bund')]['keywords_nlp'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.loc[453]['keywords_nlp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.loc[453]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build the query in the database called db\n",
    "# combined_search use AND while not combined use OR between the words\n",
    "# it returns a pandas dataframe\n",
    "def build_query(query_list, query_fields, combined_search=True):\n",
    "    query_str = []\n",
    "    if len(query_list) == 1:\n",
    "        query_word = query_list[0]\n",
    "        for field in query_fields:\n",
    "            query_str.append(f\"(db['{field}'].str.contains('{query_word}', case=False))\")\n",
    "        query_str = '|'.join(query_str)\n",
    "    else:\n",
    "        if combined_search:\n",
    "            operator = '&'\n",
    "        else:\n",
    "            operator = '|'\n",
    "        for query_word in query_list:\n",
    "            query_part = []\n",
    "            for field in query_fields:\n",
    "                if field == 'keywords_nlp':\n",
    "                    query_word = query_word.lower()\n",
    "                query_part.append(f\"(db['{field}'].str.contains('{query_word}', case=False))\")\n",
    "            query_str.append('|'.join(query_part))\n",
    "        query_str = operator.join(query_str)\n",
    "    print(query_str)\n",
    "    return eval(f\"db[{query_str}]\")\n",
    "\n",
    "def evaluate_search_results(sorted_results, test_index):\n",
    "    scores = []\n",
    "    for idx in test_index:\n",
    "        if idx in sorted_results.index.to_list():\n",
    "            scores.append(sorted_results.index.to_list().index(idx))\n",
    "        else:\n",
    "            scores.append(100)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test 1: query_words = ['drohne','einschrnkung']; test_rows = [20263, 20806, 23296]\n",
    "- Test 2: query_words = ['durchlssigkeit','Deckschichten']; test_rows = [536, 537]\n",
    "- Test 3: query_words = ['Amphibienzugstelle','fotopunkte']; test_rows = [634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644]\n",
    "- Test 4: query_words = ['radweg','zrich']; test_rows = [17557]\n",
    "- Test 4b: query_words = ['radweg','kanton schwyz']; test_rows = [11719, 12365]\n",
    "- Test 4c: query_words = ['radweg','schweiz']; test_rows = [19936, 21190]\n",
    "- Test 4d: query_words = ['veloweg','schweiz']; test_rows = [19936, 21190]\n",
    "- Test 5: query_words = ['berufsinformationszentren','kanton bern']; test_rows = [5347,5348,5349,5350]\n",
    "- Test 6: query_words = ['roemisch','pfosten','augusta raurica']; test_rows = [1781]\n",
    "- Test 7: query_words = ['wildtierkorridore','schweiz']; test_rows = [19977, 20939, 11829, 12499]\n",
    "- Test 7b: query_words = ['wildtierkorridore','kanton solothurn']; test_rows = [10398]\n",
    "- Test 8: query_words = ['Abteilung Wasserbau','Bewilligung', 'Zrich']; test_rows = [17757]\n",
    "- Test 9: query_words = ['bezirke','kanton zrich']; test_rows = [17764, 17863, 36651]\n",
    "- Test 10: query_words = ['rohstoffe','kanton schaffhausen']; test_rows = [10321]\n",
    "- Test 10b: query_words = ['rohstoffe','schweiz']; test_rows = [20701, 20702, 20698, 20700, 20412,20696,20699,20411]\n",
    "- Test 11: query_words = ['eignung','solarenergie','schweiz']; test_rows = [20289, 20790, 20290, 20791]\n",
    "- Test 11b: query_words = ['eignung','solarenergie','kanton aargau']; test_rows = [364, 365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_words = ['CH-151.1']\n",
    "test_rows = [634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644]\n",
    "query_fields = ['title','keywords','keywords_nlp', 'abstract'] # all possible results\n",
    "print(build_query(query_words,query_fields,combined_search=True).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_fields = [\"title\", \"keywords\"]# first list exact match, second list contains\n",
    "sum_score = True\n",
    "\n",
    "if sum_score:\n",
    "    for i in range(len(sorting_fields)):\n",
    "        test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words, [[],sorting_fields[0:i+1]])\n",
    "        print(np.asarray(evaluate_search_results(test_db, test_rows)).sum(), sorting_fields[0:i+1])\n",
    "    test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words, [[],['keywords']])\n",
    "    print(np.asarray(evaluate_search_results(test_db, test_rows)).sum(), 'keywords')\n",
    "    test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words)\n",
    "    print(np.asarray(evaluate_search_results(test_db, test_rows)).sum(), 'custom GeoHarvester')\n",
    "else:\n",
    "    for i in range(len(sorting_fields)):\n",
    "        test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words, [[],sorting_fields[0:i+1]])\n",
    "        print(evaluate_search_results(test_db, test_rows), sorting_fields[0:i+1])\n",
    "    test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words, [[],['keywords']])\n",
    "    print(evaluate_search_results(test_db, test_rows), 'keywords')\n",
    "    test_db = test_ranking(build_query(query_words,query_fields,combined_search=True), query_words)\n",
    "    print(evaluate_search_results(test_db, test_rows), 'custom GeoHarvester')\n",
    "print(f\"Number of test rows: {len(test_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.asarray([12, 9, 16, 8, 1, 17, 0, 14, 11, 13, 10]).sum(), np.asarray([10, 3, 8, 7, 0, 6]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_db[0:100].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cog.torque.Graph at 0x1fe4cf50850>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cog.torque import Graph #CogDB\n",
    "g = Graph(\"translation\", cog_home=\"knowledge_graph\")\n",
    "g.put(\"carta dei pericoli\",\"means\",\"Gefahrenkarte\")\n",
    "g.put(\"carte des dangers\", \"means\",\"Gefahrenkarte\")\n",
    "g.put(\"risk map\", \"means\",\"Gefahrenkarte\")\n",
    "g.put(\"Gefahrenkarte\", \"lang\", \"german\")\n",
    "g.put(\"carta dei pericoli\", \"lang\", \"italian\")\n",
    "g.put(\"carte des dangers\", \"lang\", \"french\")\n",
    "g.put(\"risk map\", \"lang\", \"english\")\n",
    "# g.put(\"carta dei pericoli\", \"means\", \"Gefahrenkarte\")\n",
    "# g.put(\"risk map\", \"synonym\",\"hazard map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.drop(\"Gefahrenkarte\", \"lang\", \"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.drop(\"risk map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the edges in out()\n",
    "g.v().tag(\"from\").out().tag(\"to\").view(\"test\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': [{'id': 'carta dei pericoli'}]}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all incoming edges for vertex\n",
    "g.v(vertex=\"italian\").inc().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "if not l:\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'carte des dangers'}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all outgoing edges for vertex \n",
    "g.v(vertex=\"Gefahrenkarte\").inc(\"means\").has('lang','french').all()['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': [{'id': 'carte des dangers'}]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all vertex with specifi edge-vertex condition outgoing\n",
    "g.v().has(\"lang\",\"french\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english', 'Gefahrenkarte']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k['id'] for k in g.v().filter(func=lambda x: x.startswith(\"italian\")).out().all()['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in g.v(\"Gefahrenkarte\").out().all()['result']:\n",
    "    print(result['id'])\n",
    "    if len(g.v(result['id']).out().all()['result']) > 0:\n",
    "        for res in g.v(result['id']).out().all()['result']:\n",
    "            print(\" \",res['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoharvester",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c031072c482ff12c1b8627010cd074fb6af482daeb686bcc3fbabf1c7aa304d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
